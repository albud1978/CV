"""
Molmo 2 ‚Äî Vision-Language Model –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ.

–ú–æ–¥–µ–ª–∏ –æ—Ç Allen Institute for AI (Ai2):
- allenai/Molmo-7B-D-0924 (7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, ~14 –ì–ë VRAM)
- allenai/Molmo-72B-0924 (72B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Ç—Ä–µ–±—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU)

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º/–≤–∏–¥–µ–æ
- Pointing (—É–∫–∞–∑–∞–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –æ–±—ä–µ–∫—Ç–æ–≤)
- –ü–æ–¥—Å—á—ë—Ç –æ–±—ä–µ–∫—Ç–æ–≤
- OCR (—á—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞)

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python src/inference/molmo2.py --image test.jpg --prompt "–û–ø–∏—à–∏ —á—Ç–æ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏" --quant 4bit
"""

import os
import sys
import argparse
import torch
from PIL import Image
from typing import Optional, List, Union

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤
try:
    from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Warning: transformers not installed. Run: pip install transformers>=4.45.0")


class Molmo2Inference:
    """
    –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–∏ Molmo 2.
    """
    
    # –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Molmo 2 (–¥–µ–∫–∞–±—Ä—å 2025)
    MODELS = {
        # Molmo 2 ‚Äî –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        "4b": "allenai/Molmo2-4B",
        "8b": "allenai/Molmo2-8B", 
        "7b": "allenai/Molmo2-O-7B",
        "video": "allenai/Molmo2-VideoPoint-4B",
        # Molmo 1 ‚Äî —Å—Ç–∞—Ä—ã–µ –º–æ–¥–µ–ª–∏ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        "molmo1-7b": "allenai/Molmo-7B-D-0924",
    }
    
    def __init__(
        self, 
        model_name: str = "7b",
        device: Optional[str] = None,
        torch_dtype: torch.dtype = torch.bfloat16,
        trust_remote_code: bool = True,
        quantization: str = "none"
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ Molmo 2.
        
        Args:
            model_name: –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ ("7b" –∏–ª–∏ "72b")
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ("cuda", "cpu", –∏–ª–∏ None –¥–ª—è –∞–≤—Ç–æ–≤—ã–±–æ—Ä–∞)
            torch_dtype: –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö (bfloat16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
            trust_remote_code: –î–æ–≤–µ—Ä—è—Ç—å –∫–æ–¥—É –º–æ–¥–µ–ª–∏ —Å HuggingFace
            quantization: –†–µ–∂–∏–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è ("none", "8bit", "4bit")
        """
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("transformers library is required. Install with: pip install transformers>=4.45.0")
        
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.torch_dtype = torch_dtype
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –º–æ–¥–µ–ª–∏
        if model_name in self.MODELS:
            self.model_id = self.MODELS[model_name]
        else:
            self.model_id = model_name  # –ü–æ–∑–≤–æ–ª—è–µ—Ç —É–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—ã–π –ø—É—Ç—å
            
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_id} (Quantization: {quantization})...")
        print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}, dtype: {torch_dtype}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
        quantization_config = None
        if quantization == "8bit":
            from transformers import BitsAndBytesConfig
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        elif quantization == "4bit":
            from transformers import BitsAndBytesConfig
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch_dtype
            )

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä + image processor)
        self.processor = AutoProcessor.from_pretrained(
            self.model_id,
            trust_remote_code=trust_remote_code,
            torch_dtype=torch_dtype,
            device_map="auto"
        )
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        model_kwargs = {
            "trust_remote_code": trust_remote_code,
            "torch_dtype": torch_dtype,
            "device_map": "auto"
        }
        if quantization_config:
            model_kwargs["quantization_config"] = quantization_config

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        print(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ AutoModelForCausalLM...")
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                **model_kwargs
            )
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: AutoModelForCausalLM –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
            print("–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ –±–∞–∑–æ–≤—ã–π AutoModel...")
            from transformers import AutoModel
            self.model = AutoModel.from_pretrained(
                self.model_id,
                **model_kwargs
            )
        
        print(f"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
    def predict(
        self,
        images: Union[str, Image.Image, List[Union[str, Image.Image]]],
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.0,
    ) -> str:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏(—è—Ö).
        
        Args:
            images: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, PIL Image, –∏–ª–∏ —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            prompt: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_new_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0 = –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
            
        Returns:
            str: –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if isinstance(images, str):
            images = [Image.open(images)]
        elif isinstance(images, Image.Image):
            images = [images]
        else:
            images = [Image.open(img) if isinstance(img, str) else img for img in images]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ RGB
        images = [img.convert("RGB") if img.mode != "RGB" else img for img in images]
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        inputs = self.processor.process(
            images=images,
            text=prompt
        )
        
        # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (self.model.device –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –±–∞–∑–∞, –Ω–æ –ø—Ä–∏ map="auto" —Ç–µ–Ω–∑–æ—Ä—ã —Ä–∞—Å–∫–∏–¥–∞–Ω—ã)
        # –ü—Ä–∏ device_map="auto" –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ to(device) –¥–ª—è inputs, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Å–∞–º–∞ —É–ø—Ä–∞–≤–ª—è–µ—Ç.
        # –ù–æ –¥–ª—è inputs –Ω–∞–¥–æ –ø—Ä–æ—Å—Ç–æ to(device) –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –∏–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞.
        # –ë–µ–∑–æ–ø–∞—Å–Ω–µ–µ –ø—Ä–æ—Å—Ç–æ to("cuda") –µ—Å–ª–∏ –º—ã –Ω–∞ GPU.
        target_device = "cuda" if torch.cuda.is_available() else "cpu"
        inputs = {k: v.to(target_device).unsqueeze(0) for k, v in inputs.items()}
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        with torch.no_grad():
            output = self.model.generate_from_batch(
                inputs,
                GenerationConfig(
                    max_new_tokens=max_new_tokens,
                    stop_strings=["<|endoftext|>"],
                    do_sample=temperature > 0,
                    temperature=temperature if temperature > 0 else None,
                ),
                tokenizer=self.processor.tokenizer
            )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        generated_tokens = output[0, inputs["input_ids"].size(1):]
        response = self.processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        return response.strip()
    
    def point(self, image: Union[str, Image.Image], object_description: str) -> dict:
        """
        –ù–∞–π—Ç–∏ –æ–±—ä–µ–∫—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –∏ –≤–µ—Ä–Ω—É—Ç—å –µ–≥–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã.
        
        Args:
            image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            object_description: –û–ø–∏—Å–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ ("—á–µ–ª–æ–≤–µ–∫ –≤ –∂–∏–ª–µ—Ç–µ", "—Ç–æ–ø–ª–∏–≤–æ–∑–∞–ø—Ä–∞–≤—â–∏–∫")
            
        Returns:
            dict: {"found": bool, "coordinates": (x, y) –∏–ª–∏ None, "response": str}
        """
        prompt = f"Point to the {object_description} in the image."
        response = self.predict(image, prompt)
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ (Molmo –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ <point x="..." y="...">)
        result = {
            "found": False,
            "coordinates": None,
            "response": response
        }
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ (–º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–¥ —Ñ–æ—Ä–º–∞—Ç Molmo 2)
        if "x=" in response and "y=" in response:
            try:
                import re
                x_match = re.search(r'x="?(\d+\.?\d*)"?', response)
                y_match = re.search(r'y="?(\d+\.?\d*)"?', response)
                if x_match and y_match:
                    result["found"] = True
                    result["coordinates"] = (float(x_match.group(1)), float(y_match.group(1)))
            except Exception:
                pass
                
        return result
    
    def describe(self, image: Union[str, Image.Image]) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        """
        return self.predict(image, "Describe this image in detail.")
    
    def count(self, image: Union[str, Image.Image], object_type: str) -> dict:
        """
        –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞.
        
        Args:
            image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            object_type: –¢–∏–ø –æ–±—ä–µ–∫—Ç–∞ ("people", "cars", "trucks")
            
        Returns:
            dict: {"count": int –∏–ª–∏ None, "response": str}
        """
        prompt = f"How many {object_type} are in this image? Answer with just the number."
        response = self.predict(image, prompt)
        
        result = {"count": None, "response": response}
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —á–∏—Å–ª–∞
        import re
        numbers = re.findall(r'\d+', response)
        if numbers:
            result["count"] = int(numbers[0])
            
        return result
    
    def ocr(self, image: Union[str, Image.Image]) -> str:
        """
        –ü—Ä–æ—á–∏—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.
        """
        return self.predict(image, "Read all text visible in this image.")
    
    def answer(self, image: Union[str, Image.Image], question: str) -> str:
        """
        –û—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.
        """
        return self.predict(image, question)


def main():
    parser = argparse.ArgumentParser(description="Molmo 2 Inference")
    parser.add_argument("--image", type=str, required=True, help="–ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é")
    parser.add_argument("--prompt", type=str, default="Describe this image in detail.", 
                        help="–¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
    parser.add_argument("--model", type=str, default="4b", 
                        choices=["4b", "8b", "7b", "video", "molmo1-7b"],
                        help="–ú–æ–¥–µ–ª—å: 4b, 8b (Molmo2), 7b (Molmo2-O), video (VideoPoint)")
    parser.add_argument("--task", type=str, default="qa", 
                        choices=["qa", "describe", "point", "count", "ocr"],
                        help="–¢–∏–ø –∑–∞–¥–∞—á–∏")
    parser.add_argument("--object", type=str, default="person",
                        help="–û–±—ä–µ–∫—Ç –¥–ª—è point/count –∑–∞–¥–∞—á")
    parser.add_argument("--quant", type=str, default="none", 
                        choices=["none", "8bit", "4bit"], 
                        help="–†–µ–∂–∏–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (—ç–∫–æ–Ω–æ–º–∏—è VRAM)")
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞
    if not os.path.exists(args.image):
        print(f"Error: —Ñ–∞–π–ª {args.image} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        sys.exit(1)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    model = Molmo2Inference(model_name=args.model, quantization=args.quant)
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏
    if args.task == "describe":
        result = model.describe(args.image)
        print(f"\nüìù –û–ø–∏—Å–∞–Ω–∏–µ:\n{result}")
        
    elif args.task == "point":
        result = model.point(args.image, args.object)
        print(f"\nüìç Pointing:")
        print(f"   –ù–∞–π–¥–µ–Ω: {result['found']}")
        print(f"   –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: {result['coordinates']}")
        print(f"   –û—Ç–≤–µ—Ç: {result['response']}")
        
    elif args.task == "count":
        result = model.count(args.image, args.object)
        print(f"\nüî¢ –ü–æ–¥—Å—á—ë—Ç:")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {result['count']}")
        print(f"   –û—Ç–≤–µ—Ç: {result['response']}")
        
    elif args.task == "ocr":
        result = model.ocr(args.image)
        print(f"\nüìñ OCR:\n{result}")
        
    else:  # qa
        result = model.answer(args.image, args.prompt)
        print(f"\nüí¨ –û—Ç–≤–µ—Ç:\n{result}")


if __name__ == "__main__":
    main()
